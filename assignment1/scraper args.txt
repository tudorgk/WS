optional arguments:
  -h, --help            show this help message and exit
  -m {http,selenium,http-async}, --scrape-method {http,selenium,http-async}
                        The scraping type. There are currently three types:
                        "http", "selenium" and "http-async". "Http" scrapes
                        with raw http requests, whereas "selenium" uses the
                        selenium framework to remotely control browsers'.
                        "http-async" makes use of gevent and is well suited
                        for extremely fast and explosive scraping jobs. You
                        may search more than 1000 requests per second if you
                        have the necessary number of proxies available.
  -q KEYWORD, --keyword KEYWORD
                        The search keyword to scrape for. If you need to
                        scrape multiple keywords, use the --keyword-file flag
  --keyword-file KEYWORD_FILE
                        Keywords to search for. One keyword per line. Empty
                        lines are ignored. Alternatively, you may specify the
                        path to an python module (must end with the .py
                        suffix) where the keywords must be held in a
                        dictionary with the name "scrape_jobs".
  -o- OUTPUT_FILENAME, --output-filename OUTPUT_FILENAME
                        The name of the output file. If the file ending is
                        "json", write a json file, if the ending is "csv",
                        write a csv file.
  --shell               Fire up a shell with a loaded sqlalchemy session.
  -n NUM_RESULTS_PER_PAGE, --num-results-per-page NUM_RESULTS_PER_PAGE
                        The number of results per page. Must be smaller than
                        100, by default 50 for raw mode and 10 for selenium
                        mode. Some search engines ignore this setting.
  -p NUM_PAGES_FOR_KEYWORD, --num-pages-for-keyword NUM_PAGES_FOR_KEYWORD
                        The number of pages to request for each keyword. Each
                        page is requested by a unique connection and if
                        possible by a unique IP (at least in "http" mode).
  -z NUM_WORKERS, --num-workers NUM_WORKERS
                        This arguments sets the number of browser instances
                        for selenium mode or the number of worker threads in
                        http mode.
  -t SEARCH_TYPE, --search-type SEARCH_TYPE
                        The searchtype to launch. May be normal web search,
                        image search, news search or video search.
  --proxy-file PROXY_FILE
                        A filename for a list of proxies (supported are HTTP
                        PROXIES, SOCKS4/5) with the following format:
                        "Proxyprotocol (proxy_ip|proxy_host):Port\n" Example
                        file: socks4 127.0.0.1:99 socks5 33.23.193.22:1080
  --config-file CONFIG_FILE
                        The path to the configuration file for GoogleScraper.
                        Normally you won't need this, because GoogleScrape
                        comes shipped with a thoroughly commented
                        configuration file named `config.cfg`
  --simulate            If this flag is set, the scrape job and its estimated
                        length will be printed.
  -v VERBOSITY, --verbosity VERBOSITY
                        The verbosity of GoogleScraper output. 0: no ouput, 1:
                        most necessary info, summary (no results), 2: detailed
                        scraping info (still without results), 3: show parsed
                        results:, > 3: Degbug info.
  --view-config         Print the current configuration to stdout. You may use
                        it to create and tweak your own config file from it.
  --version             Prints the version of GoogleScraper
  --clean               Cleans all stored data. Please be very careful.
  -c EXTENDED_CONFIG, --extended-config EXTENDED_CONFIG
                        Pass additional configuration to GoogleScraper. The
                        section ("GLOBAL" or "SCRAPING" for example) is not
                        needed. Example: "--extended-config 'search_offset: 1
                        | clean_cache_files: False'"
  --mysql-proxy-db MYSQL_PROXY_DB
                        A mysql connection string for proxies to use. Format:
                        mysql://<username>:<password>@<host>/<dbname>. Has
                        precedence over proxy files.
  -s SEARCH_ENGINES, --search-engines SEARCH_ENGINES
                        What search engines to use (See GoogleScraper --config
                        for the all suported). If you want to use more than
                        one at the same time, just separate with commatas:
                        "google, bing, yandex". If you want to use all search
                        engines that are available, give '*' as argument.

